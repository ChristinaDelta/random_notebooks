{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING ALGORITHMS PART 1.\n",
    "\n",
    "### Linear Regression \n",
    "\n",
    "I will be using information and examples based on the **TensorFlow 2.0** course provided by the FreeCodeCamp and the TensorFlow documentation [doc](https://www.tensorflow.org/tutorials/quickstart/beginner).\n",
    "\n",
    "First, let's import the requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE DATA\n",
    "\n",
    "The dataset I will be focusing on here is the titanic dataset. It has tons of information about each passanger on the ship. Our first step is always to understand the data and explore it. So, let's do that!\n",
    "What we are going to do, is to **predict the chance of survival** using the information provided by the tiatanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset that will be used here\n",
    "dftrain = pd.read_csv('./train.csv')\n",
    "dfeval = pd.read_csv('./eval.csv')\n",
    "y_train = dftrain.pop('survived') # pop/extract a specific column. This column is our y variable.\n",
    "y_eval = dfeval.pop('survived') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```pd.read_csv()``` method loads the dataset as a *pandas dataframe*. A dataframe is kind of like a table.\n",
    "We can have a look at the table using simple commands such as ```dfname.head()``` where **dfname** is the name that you have given to your dataframe during loading. If the parenthesis is left blank then by default it will visualize the first 5 rows. In this dataset the y (dependent variable) is the **survived** column which was extracted and stored in a new variable(y_train, y_eval). The rest are indepenent variables x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataframe\n",
    "dftrain.head(10)\n",
    "#y_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loc method to locate a specific row. Let's locate the 1st row in the dftrain dataframe and \n",
    "# the y_train dataframe\n",
    "print(dftrain.loc[0], y_train.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line of the output above *Name: 0, dtype: object 0* is the output of ```print(y_train.loc[0])``` and it only means that this 1st person (remember I located the 1st row) did not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to look at a specific column:\n",
    "print(dftrain[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the discriptives\n",
    "dftrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```dfname.describe()``` method gives us some of the discriptive statistics od the dataframe such as, mean, standard deviation, etc. \n",
    "\n",
    "\n",
    "We can also look at the dataframe's shape. NumPy arrays are not the only ones with shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dftrain.shape\n",
    "dfeval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is just 2 values. The first value (627) is the number of rows and the 2nd value (9) is the number of columns/features in this dataframe. \n",
    "\n",
    "Let's take a look at the survival column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(10) # in the output 0=did not survive and 1=survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's make a few graphs of the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.age.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.sex.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['class'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick visualization we see that:\n",
    "* most passengers were between 20 and 30 years old\n",
    "* most passengers were males\n",
    "* most passengers were in the 3rd class\n",
    "* most passengers that survived were females\n",
    "\n",
    "we used this visualization to get some idea of what kind of data we have, how we are going to further analyze the dataset and what kind of results should we expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data\n",
    "\n",
    "We have loaded two different datasets. This is because when we train models we need teo different sets of dat: one for **training** and one for **testing**. \n",
    "In a few words, we feed our model with the **training** data, so that it can learn and develop. Normally, a training dataset is larger than the testing dataset.\n",
    "\n",
    "We use the **testing** dataset to evaluate the model and see how it performs. These two datasets should not be the same. **WHY?**\n",
    "\n",
    "The whole point is for our model to be able to do predictions with new data, data that it hasn't seen before. If we evaluate the model with data that it has already seen then its performance wouldn't be really accurate. What if the model simply memorized the training data? \n",
    "This is why the testing and training datasets need to be different.\n",
    "\n",
    "## Preparing our dataset\n",
    "\n",
    "### Feature columns\n",
    "\n",
    "In the beggining I imported something called **fc**, which stands for **feature columns**. \n",
    "In our dataset we have two types of information: Numerical and categorical. In the next block of code I will need to loop through these types of information to create feature columns. Feature columns is just what we feed our linear estimator with, to make predictions.\n",
    "\n",
    "Numerical information can be found in all the columns that contain numbers:\n",
    "* age\n",
    "* fare\n",
    "\n",
    "Categorical information is the information that can be put in categories and/or is not numerical:\n",
    "* sex\n",
    "* simblings & spouces \n",
    "* deck\n",
    "* alone\n",
    "\n",
    "Before we continue with the training, we must first convert the categorical data into numerical. The way to do this is by encoding or replacing each category with an integer (for example, male = 1, female = 0).\n",
    "We are lucky that TensorFlow can handle that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your categorical and numerical columns:\n",
    "categorical_columns = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "numerical_columns = ['age', 'fare']\n",
    "\n",
    "# convert the categorical into numerical\n",
    "feature_columns = [] # create black list to store the feature columns\n",
    "for fname in categorical_columns:\n",
    "    vocabulary = dftrain[fname].unique() # get the unique values from a given categorical column \n",
    "    # store these unique values in the feature column\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(fname, vocabulary))\n",
    "\n",
    "    # it's easier to create a feature column with (already) numerical values, we just give the feature name \n",
    "    # (name of a given column) and the datatype (e.g. float32)\n",
    "for fname in numerical_columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(fname, dtype = tf.float32))\n",
    "    \n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the block of code above, I just created a list of features that are used in the dataset. \n",
    "The lines of code inside the ```append()``` attribute create an object that the model will then use to map unique string values (e.g. male and female) to integers (e.g. 0 and 1). For more information on feature columns see [link](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list?version=stable)\n",
    "\n",
    "Take a look at the unique values first, and then at the different categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at all the unique values stored in vocabulary\n",
    "dftrain[fname].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['sex'].unique()\n",
    "dftrain['embark_town'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in numerical_columns:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is our model trained?\n",
    "\n",
    "The way we train a model is by feeding it with information (with data). For this model, data will not be fed all at once but in batches of 32 entries. We'll feed our model several times depending on the number of **epochs**. \n",
    "\n",
    "This dataset is small so we wouldn't normally need to train the model in batches of data, but if someone has tons of data to train a model with, this is when the batches are quite needed. \n",
    "\n",
    "### What is an epoch?\n",
    "\n",
    "Epoch is just one stream of the entire dataset. The number of epochs is the number of times the model will see the entire dataset. We are using a lot of repeats of our dataset (a lot of epochs) in different order and the reason is because we want to make sure that the model will look at the (same) data a lot of times but from *different views* and start making patterns. The idea is that the more our model looks at the data, the easier it is to make predictions.\n",
    "\n",
    "#### the problem of overffiting \n",
    "Sometimes it may be the case that the model sees the same datapoints too many times, at the point where it just memorizes them instead of making patterns using the data we feed it with. In such a case, when we give our model some new data (some testing data) the performance is not really accurate (I think this is a common problem in classification). \n",
    "\n",
    "In order to prevent that from happening we can start with a small number of epochs and increase them later on (if needed).\n",
    "\n",
    "### Input function\n",
    "Given that our model will see the entire dataset many times and in small batches, we first need to create  an **input function**. This function will just define how our dataset will be split into batches of 32 entries at each epoch.\n",
    "\n",
    "The TensorFlow model we are using requires that the data we give it comes  as a ```tf.data.Dataset``` object. So, the input function that we'll create will convert the pandas df into that object. The input function comes directly from the TensorFlow documentation, see [link](https://www.tensorflow.org/tutorials/estimator/linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_func(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function(): # inner function, this will be returned\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # create a tf.data.Dataset object with the data and label\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000) # randomize the order of the data\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs) # split the dataset into batches of 32 and repeat this process n times (depending on epochs)\n",
    "        # return a batch of the dataset\n",
    "        return(ds)\n",
    "    # return a function object to use\n",
    "    return(input_function)\n",
    "\n",
    "# now call the input function that was returned to us to get a dataset object we'll feed to the model\n",
    "train_input_func = make_input_func(dftrain, y_train)\n",
    "eval_input_func = make_input_func(dfeval, y_eval, num_epochs =1, shuffle = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the code above. We created two functions (a function within a function). The exterior function ```def make_input_func()``` needs a few parameters:\n",
    "\n",
    "* data_df = train data (our x variables) or features\n",
    "* label_df = data that we want our model to predict (the y variable)\n",
    "* num_epochs = number of repeats that the model will see our dataset\n",
    "* shuffle = true or false. Are we going to randomize the data before passing it to the model?\n",
    "* batch_size = number of entries at each epoch.\n",
    "\n",
    "In the inner function ```def input_function()``` we create an object dataset (ds) by passing a dictionary representation of our x values and y values.\n",
    "Now, the inner function returns the dataset object that we created and the exterior function returns the inner function for us. So, what the exterior function does, is that it makes the **imput function**. When we want to call the input function, we are actually going to call the exterior ```def make_input_func()``` because this one will return to us the actual input function.\n",
    "\n",
    "### Calling the input function \n",
    "\n",
    "We call the same function to create our object dataset for both training and testing. The diffrence is that for testing we don't need to shuffle our data (obviously!), so it is set to **false** and we just want the model to see the data once because it has already been trained.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now create the Model\n",
    "\n",
    "Here, I will use the linear estimator to make use of the linear regression algorithm.\n",
    "\n",
    "It's pretty easy to create a model. I'll be using the **LinearClassifier** object from the **estimator** module of TensorFlow by passing the *feature_columns* list created before.\n",
    "\n",
    "*More information about estimators can be found in ML_algorithms Part2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns = feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Training the model is as easy as passing the input function created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_est.train(train_input_func) # and that's it!\n",
    "results = linear_est.evaluate(eval_input_func) # evaluate the model by testing it with some other data (testing data)\n",
    "\n",
    "clear_output() # clear the output created while we train the model\n",
    "print(results['accuracy']) # print the accuracy level of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So essentially what I've done above is:\n",
    "\n",
    "* train the model by passing it the **object dataset** created earlier using the ```def input_function()``` function,\n",
    "* evaluate the model using the **object dataset** created earlier for evaluation/testing using the same function\n",
    "* print the accuracy of the model\n",
    "\n",
    "Now, let's take a look at the **results** we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the **results** is just a dictionary object with a banch of keys-values of statistical information related to the perfrormance of the model. This information may not tell us much at first, but if we want to access a specific piece of information we just define it as above: ```print(results['specific_key'])```, where specific key is the piece of information we want to access (e.g. accuracy). \n",
    "\n",
    "### What is this Accuracy key?\n",
    "\n",
    "When I trained and evaluated the model the next thing I did was to check the accuracy of the model. But what exactly is this accuarcy? \n",
    "It is simply the *comparison* between the results of the evaluation of the model (the predictions it made based on the tarining data) vs the actual y values of the dataset (the survival column that we extracted in the beggining).  \n",
    "\n",
    "Now, notice that if we run the model again the accuracy will probably change. This is because we shuffle our dataset, we put it in different order and based on the order the dataset is seen by the model it is being treated differently. Also, if we change the number of epochs (e.g. 20 epochs instead of 10) the accuracy again will probably change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model to make predictions\n",
    "\n",
    "Up until now, what I've done was to create the feature column list (our input data), make the input function that converts the pandas dataframes (our data) into an object dataset, create the linear model, train it and lastly test it. \n",
    "Now it's time to actually use the model to make predictions. \n",
    "So, with this silly titanic dataset (provided in the TensorFlow documentation) what we want is to predict the probabiltiy of survival (of a passenger) based on some information such as age, sex, deck, etc... We can use the ```.predict()``` method to estimate this probability: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dicts = list(linear_est.predict(eval_input_func)) # convert the dataset object into a list of dictionaries to later loop through it.\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts]) # take only the probabilities keys to plot them \n",
    "\n",
    "probs.plot(kind = 'hist', bins = 20, title = 'predicted probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pred_dicts** is a list of dictionaries that represent information for every single prediction that the model has made. The dfeval dataframe has 264 values, so pred_dicts also has 264 dictionaries, one prediction for every datapoint). If we want to look at a specific dictionary in this list we just need to provide its index: ```print(pred_dicts[263])```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediciton for a specific datapoint/subject etc..\n",
    "print(pred_dicts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within each prediction dictionary what we are interested in is the 'probabilties' key.\n",
    "print(pred_dicts[1]['probabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have 2 possible outcomes (either someone survived (1) or did not survive(0)) we have 2 probabilities. The first probabilty is for **0 = did not survive** and the second for **1 = survived**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the survived probability only:\n",
    "print(pred_dicts[1]['probabilities'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare all the information of a specific person/datapoint with their probability of survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfeval.loc[1]) # second person/datapoint, x values\n",
    "print(y_eval.loc[1]) # actual y value. \n",
    "print(pred_dicts[1]['probabilities'][1]) # second dictionary/prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info above shows that this person was a male 54 years old, did not survive and the prediction of survival (of our model) was 0.32 (pretty low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
